{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T13:46:47.250443Z",
     "iopub.status.busy": "2024-02-11T13:46:47.249957Z",
     "iopub.status.idle": "2024-02-11T13:46:50.707965Z",
     "shell.execute_reply": "2024-02-11T13:46:50.707007Z"
    }
   },
   "outputs": [],
   "source": [
    "# set working directory\n",
    "from random import SystemRandom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "# from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from torch._C import float32\n",
    "import argparse\n",
    "from asyncio.log import logger\n",
    "import os, math\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Importing matplotlib and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "# import tabulate\n",
    "\n",
    "import utils\n",
    "from utils import *\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T13:46:50.713201Z",
     "iopub.status.busy": "2024-02-11T13:46:50.712347Z",
     "iopub.status.idle": "2024-02-11T13:46:50.786998Z",
     "shell.execute_reply": "2024-02-11T13:46:50.786134Z"
    }
   },
   "outputs": [],
   "source": [
    "MINI_BATCH = 64\n",
    "EPOCHS = 1000\n",
    "LOAD = None\n",
    "SEED = 42\n",
    "REPEAT = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Set device to GPU\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Set device to CPU\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "param_grid = {\n",
    "            'drop_rate': [0.1],\n",
    "            'hidden_sizes':[[50], [100], [100, 100]],# [[50], [50, 50], [50, 100], [100], [100, 100]],\n",
    "            'head_sizes':[[50], [100]],# [[50], [100]],\n",
    "            'lr':[0.0001, 0.0005]\n",
    "        }\n",
    "# params_risk = {\n",
    "#             'drop_rate': 0.05,\n",
    "#             'hidden_sizes':[100],# [[50], [50, 50], [50, 100], [100], [100, 100]],\n",
    "#             'head_sizes':[50],# [[50], [100]],\n",
    "#             'lr':0.0001\n",
    "#         }\n",
    "# params_censoring = params_risk \n",
    "############################################\n",
    "# initilising wandb\n",
    "# wandb.init(project='SeletionBML', entity=\"jmdvinodjmd\")\n",
    "wandb.init(mode=\"disabled\")\n",
    "wandb.run.name = 'SB'\n",
    "makedirs('./results/')\n",
    "experimentID = LOAD\n",
    "if experimentID is None:\n",
    "    experimentID = int(SystemRandom().random()*100000)\n",
    "# checkpoint\n",
    "ckpt_path = os.path.join('./results/checkpoints/IPW_model.ckpt')\n",
    "makedirs('./results/checkpoints/')\n",
    "# set logger\n",
    "log_path = os.path.join(\"./results/logs/\" + \"exp_IPW_\" + str(experimentID) + \".log\")\n",
    "makedirs(\"./results/logs/\")\n",
    "logger = get_logger(logpath=log_path, filepath=\"exp_IPW_\" + str(experimentID) + \".log\", displaying=False)\n",
    "logger.info(\"Experiment \" + str(experimentID))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T13:46:50.792588Z",
     "iopub.status.busy": "2024-02-11T13:46:50.792108Z",
     "iopub.status.idle": "2024-02-11T13:46:50.822874Z",
     "shell.execute_reply": "2024-02-11T13:46:50.822179Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(data, weights, params_risk, repeat=1):\n",
    "    [X_train, y_train, s_train, X_val, y_val, s_val, X_test, y_test, s_test] = data\n",
    "    W_train, W_val = weights\n",
    "\n",
    "    # data for risk prediction\n",
    "    loader_train_br, input_size = get_loaders([X_train[s_train==0], y_train[s_train==0]], weights=W_train[s_train==0], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "    loader_val_br, _ = get_loaders([X_val[s_val==0], y_val[s_val==0]], weights=W_val[s_val==0], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "    loader_test_ur, _ = get_loaders([X_test, y_test], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "\n",
    "    # repeating experiment for a given number of times\n",
    "    results_risk = {}\n",
    "    for i in range(repeat):\n",
    "        logger.info('Repeating: ' + str(i+1))\n",
    "        results_risk[i] = {}\n",
    "        #############################\n",
    "        # train risk prediction model\n",
    "        model_risk, optimizer, criterion = create_model('IPW', params_risk, input_size, output_size=1, device=device)\n",
    "        early_stopping = EarlyStopping(patience=10, path=ckpt_path, verbose=True, logger=logger)\n",
    "        logger.info(model_risk)\n",
    "        wandb.watch(model_risk)\n",
    "        # train\n",
    "        model_risk = train_model(model_risk, 'IPW', loader_train_br, loader_val_br, optimizer, criterion, early_stopping, logger, epochs=EPOCHS, plot=False, wandb=wandb)\n",
    "        # evaluate\n",
    "        auroc_vb, _ = evaluate_model('Val', loader_val_br, model_risk, 'IPW', criterion, logger, -1, device, wandb)\n",
    "        auroc_tu, _ = evaluate_model('Test', loader_test_ur, model_risk, 'MLP', criterion, logger, -1, device, wandb)\n",
    "        logger.info('Risk Prediction: biased Val AUROC:' + str(auroc_vb['Val AUROC']) + ' unbiased Test AUROC:' + str(auroc_tu['Test AUROC']))\n",
    "        results_risk[i] = {'R-Val AUROC':auroc_vb['Val AUROC'], 'R-Test AUROC':auroc_tu['Test AUROC']}\n",
    "\n",
    "        ############################\n",
    "\n",
    "    return results_risk\n",
    "\n",
    "def study_effect(data_name, file_name, results_file, r, c, n, search_param=False):\n",
    "    ''' \n",
    "    This function is used to study effect of (riks rate, dataset size etc.).\n",
    "    It expects a set of datasets with some variations.\n",
    "    '''\n",
    "    logger.info('\\n\\n-------------N:'+str(n)+'--Risk Rate:' + str(r)+'--Censoring Rate:' + str(c)+'-------------------------.')\n",
    "\n",
    "    results_sizes = {}\n",
    "    for ni in n:\n",
    "        for ci in c:\n",
    "            for ri in r:\n",
    "                # load data dictionary\n",
    "                data_dict = get_data_dict(file_name, [ri], [ci], [ni])\n",
    "\n",
    "                logger.info('-----Running for Size:'+str(ni)+'--Risk Rate:' + str(ri)+'--Censoring Rate:' + str(ci)+'\\n-----------')\n",
    "                [X_train, y_train, s_train, X_val, y_val, s_val, X_test, y_test, s_test] = data_dict[str(ni)+'R'+str(ri)+'C'+str(ci)]\n",
    "                                \n",
    "                ##############################\n",
    "                # Reading hyperparameters from the JSON file\n",
    "                with open('best_hyperparams.json', 'r') as json_file:\n",
    "                    best_hyperparams = json.load(json_file)\n",
    "                if ('IPW-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci) not in best_hyperparams) or search_param:\n",
    "                    # hyperparameter tuning\n",
    "                    logger.info('Finding best hyperparams.')\n",
    "                    loader_train_c, input_size = get_loaders([X_train, s_train], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "                    loader_val_c, _ = get_loaders([X_val, s_val], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "                    params_censoring, best_score, results = grid_search_MLP('MLP', loader_train_c, loader_val_c, input_size, ckpt_path, param_grid, EPOCHS, logger, wandb, device)\n",
    "                    logger.info('Hyperparam tuning for censoring prediction:')\n",
    "                    logger.info(results)\n",
    "\n",
    "                    # first generate weights for hyperparam tuning\n",
    "                    model_sensoring, optimizer, criterion = create_model('MLP', params_censoring, input_size, output_size=1, device=device)\n",
    "                    early_stopping = EarlyStopping(patience=10, path=ckpt_path, verbose=True, logger=logger)\n",
    "                    logger.info(model_sensoring)\n",
    "                    wandb.watch(model_sensoring)\n",
    "                    model_sensoring = train_model(model_sensoring, 'MLP', loader_train_c, loader_val_c, optimizer, criterion, early_stopping, logger, epochs=EPOCHS, plot=False, wandb=wandb)\n",
    "                    W_train = model_sensoring(torch.tensor(X_train, dtype=torch.float).to(device)).detach().cpu().numpy().astype(float)\n",
    "                    W_train = (s_train.sum()/s_train.shape[0])/W_train\n",
    "                    logger.info('train weights range:' +str(W_train.min())+' ' + str(W_train.max()))\n",
    "                    #clip large weights\n",
    "                    W_train = np.clip(W_train, 0, 100)\n",
    "                    W_val = model_sensoring(torch.tensor(X_val, dtype=torch.float).to(device)).detach().cpu().numpy().astype(float)\n",
    "                    W_val = (s_val.sum()/s_val.shape[0])/W_val\n",
    "                    logger.info('val weights range:' +str(W_val.min())+' ' + str(W_val.max()))\n",
    "                    W_val = np.clip(W_val, 0, 100)\n",
    "\n",
    "                    # tuning of IPW risk estimator\n",
    "                    loader_train_br, input_size = get_loaders([X_train[s_train==0], y_train[s_train==0]], weights=W_train[s_train==0], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "                    loader_val_br, _ = get_loaders([X_val[s_val==0], y_val[s_val==0]], weights=W_val[s_val==0], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "                    params_risk, best_score, results = grid_search_MLP('IPW', loader_train_br, loader_val_br, input_size, ckpt_path, param_grid, EPOCHS, logger, wandb, device)\n",
    "                    logger.info('Hyperparam tuning for risk prediction:')\n",
    "                    logger.info(results)\n",
    "\n",
    "                    best_hyperparams['IPW-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci)] = {'params_risk': params_risk,\n",
    "                                                                          'params_censoring': params_censoring}\n",
    "                    # save best params\n",
    "                    with open('best_hyperparams.json', 'w') as json_file:\n",
    "                        json.dump(best_hyperparams, json_file)\n",
    "                    \n",
    "                else:\n",
    "                    logger.info('Accessing the existing best hyperparams.')\n",
    "                    params_risk = best_hyperparams['IPW-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci)]['params_risk']\n",
    "                    params_censoring = best_hyperparams['IPW-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci)]['params_censoring']\n",
    "                    # generate weights\n",
    "                loader_train_c, input_size = get_loaders([X_train, s_train], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "                loader_val_c, _ = get_loaders([X_val, s_val], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "                model_sensoring, optimizer, criterion = create_model('MLP', params_censoring, input_size, output_size=1, device=device)\n",
    "                early_stopping = EarlyStopping(patience=10, path=ckpt_path, verbose=True, logger=logger)\n",
    "                logger.info(model_sensoring)\n",
    "                wandb.watch(model_sensoring)\n",
    "                model_sensoring = train_model(model_sensoring, 'MLP', loader_train_c, loader_val_c, optimizer, criterion, early_stopping, logger, epochs=EPOCHS, plot=False, wandb=wandb)\n",
    "                W_train = model_sensoring(torch.tensor(X_train, dtype=torch.float).to(device)).detach().cpu().numpy().astype(float)\n",
    "                W_train = (s_train.sum()/s_train.shape[0])/W_train\n",
    "                logger.info('train weights range:' +str(W_train.min())+' ' + str(W_train.max()))\n",
    "                #clip large weights\n",
    "                W_train = np.clip(W_train, 0, 100)\n",
    "                W_val = model_sensoring(torch.tensor(X_val, dtype=torch.float).to(device)).detach().cpu().numpy().astype(float)\n",
    "                W_val = (s_val.sum()/s_val.shape[0])/W_val\n",
    "                logger.info('val weights range:' +str(W_val.min())+' ' + str(W_val.max()))\n",
    "                W_val = np.clip(W_val, 0, 100)\n",
    "\n",
    "                ################################\n",
    "                # run experiments and repeat for given number of times\n",
    "                data = [X_train, y_train, s_train, X_val, y_val, s_val, X_test, y_test, s_test]\n",
    "                results = experiment(data, [W_train, W_val], params_risk, repeat=REPEAT)\n",
    "                logger.info('\\n\\nBest params for risk:\\n' + str(params_risk))\n",
    "                logger.info('Best params for censoring:\\n' + str(params_censoring))\n",
    "                logger.info(results)\n",
    "                results_sizes[str(ni)+'R'+str(ri)+'C'+str(ci)] = results\n",
    "\n",
    "                # save results\n",
    "                dict_to_file(results_file, results_sizes)\n",
    "                ################################\n",
    "\n",
    "    logger.info('\\n\\n------------------- Experiments ended-------------------.\\n'+str(results_sizes)+'\\n------------------------------------------------\\n\\n')\n",
    "\n",
    "    return results_sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T13:46:50.839048Z",
     "iopub.status.busy": "2024-02-11T13:46:50.838718Z",
     "iopub.status.idle": "2024-02-11T13:46:50.842239Z",
     "shell.execute_reply": "2024-02-11T13:46:50.841406Z"
    }
   },
   "outputs": [],
   "source": [
    "results_sizes = study_effect('synthetic', 'selection_bias_data.pkl', 'results_IPW', r=[.05, .1, .2, .3, .4], c=[.05, .1, .2, .3, .4], n=[1000, 2000, 3000, 4000, 5000], search_param=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T13:46:50.859829Z",
     "iopub.status.busy": "2024-02-11T13:46:50.859263Z",
     "iopub.status.idle": "2024-02-12T03:27:37.472079Z",
     "shell.execute_reply": "2024-02-12T03:27:37.471257Z"
    }
   },
   "outputs": [],
   "source": [
    "results_sizes = study_effect('diabetes', 'diabetes_bias_data.pkl', 'results_IPW-diabetes', r=[.05, .1, .2, .3, .4], c=[.05, .1, .2, .3, .4], n=[25000, 10000, 5000, 2000, 1000], search_param=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T03:27:37.489377Z",
     "iopub.status.busy": "2024-02-12T03:27:37.489035Z",
     "iopub.status.idle": "2024-02-12T03:27:37.493048Z",
     "shell.execute_reply": "2024-02-12T03:27:37.492402Z"
    }
   },
   "outputs": [],
   "source": [
    "results_sizes = study_effect('covid', 'covid_bias_data.pkl', 'results_IPW-covid', r=[.05, .1, .2, .3, .4], c=[.05, .1, .2, .3, .4], n=[15000, 10000, 5000, 2000, 1000], search_param=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
