{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from scipy.special import expit\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data for Selection Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, y, c, n, p_y, p_c, dataframe=False):\n",
    "    '''\n",
    "        new: filter data to have total n data points where it has event rate (in uncensored) as p_y and censoring rate in total as p_c\n",
    "    '''\n",
    "    df = pd.DataFrame(data)\n",
    "    df['y'] = y\n",
    "    df['c']= c\n",
    "\n",
    "    num_c = int(n * p_c)\n",
    "    num_y = int((n-num_c) * p_y)\n",
    "    # print(num_c, num_y)\n",
    "\n",
    "    df_c = df.loc[df['c']==1].iloc[:num_c,:]\n",
    "\n",
    "    df_y1 = df.loc[(df['c'] == 0) & (df['y'] == 1)].iloc[:num_y,:]\n",
    "    df_y0 = df.loc[(df['c'] == 0) & (df['y'] == 0)].iloc[:(n-num_y-num_c),:]\n",
    "\n",
    "    results = pd.concat([df_c, df_y1, df_y0], axis=0, ignore_index=True)\n",
    "    results = shuffle(results)\n",
    "\n",
    "    df = results.drop(columns=['y', 'c'])\n",
    "\n",
    "    if dataframe:\n",
    "        y = results['y']\n",
    "        c = results['c']\n",
    "        X = df\n",
    "    else:\n",
    "        y = results['y'].values\n",
    "        c = results['c'].values\n",
    "        X = df.values\n",
    "\n",
    "    return X, y, c\n",
    "\n",
    "def check_least_populated(X, y, c):\n",
    "    if len(X[(y==0) & (c==0)])==1:\n",
    "        con = ((y==0) & (c==0))\n",
    "        X = X[~con].reset_index(drop=True)\n",
    "        y = y[~con].reset_index(drop=True)\n",
    "        c = c[~con].reset_index(drop=True)\n",
    "    elif len(X[(y==0) & (c==1)])==1:\n",
    "        con = ((y==0) & (c==1))\n",
    "        X = X[~con].reset_index(drop=True)\n",
    "        y = y[~con].reset_index(drop=True)\n",
    "        c = c[~con].reset_index(drop=True)\n",
    "    elif len(X[(y==1) & (c==0)])==1:\n",
    "        con = ((y==1) & (c==0))\n",
    "        X = X[~con].reset_index(drop=True)\n",
    "        y = y[~con].reset_index(drop=True)\n",
    "        c = c[~con].reset_index(drop=True)\n",
    "    elif len(X[(y==1) & (c==1)])==1:\n",
    "        con = ((y==1) & (c==1))\n",
    "        X = X[~con].reset_index(drop=True)\n",
    "        y = y[~con].reset_index(drop=True)\n",
    "        c = c[~con].reset_index(drop=True)\n",
    "\n",
    "    return X, y, c\n",
    "\n",
    "def generate_classification_data(m, c, num_samples, num_features, nonlinear=True, nlband=10, flip_y=0.01):\n",
    "    # Generate features data\n",
    "    features = np.random.uniform(-10, 10, size=(num_samples, num_features))\n",
    "\n",
    "    # generate labels\n",
    "    linear_output1 = np.dot(features**1, m) + c\n",
    "    labels = (linear_output1>0).astype(int)\n",
    "\n",
    "    # Randomly replace labels\n",
    "    # print('before:', labels.sum())\n",
    "    if flip_y >= 0.0:\n",
    "        flip_mask = np.random.uniform(size=labels.shape[0]) < flip_y\n",
    "        labels[flip_mask] = np.random.randint(2, size=flip_mask.sum())\n",
    "    # print('after:', labels.sum())\n",
    "    \n",
    "    # generate censoring\n",
    "    m[1] *= 10\n",
    "    c = -5\n",
    "    linear_output2 = np.dot(features**1, m) + c\n",
    "    sensoring = ((linear_output1>0) ^ (linear_output2>0)).astype(int)\n",
    "\n",
    "    return features, labels, sensoring\n",
    "\n",
    "def get_dataset_combinations(risk_rate=[.1, .05], censoring_rate=[.1, .05], n=[2000, 1000], test_size=0.2):\n",
    "  '''\n",
    "    This will generate data for different combinations of risk, censoring and dataset size.\n",
    "        n - is total number of data points\n",
    "        censoring_rate - rate of censoring and calculated on n\n",
    "        risk_rate - event rate and calculated on uncensored data (i.e., after removing censored units)\n",
    "        test_size - test and validation split fraction\n",
    "\n",
    "    '''\n",
    "  data_dict = {}\n",
    "\n",
    "  np.random.seed(42)  # Set seed for reproducibility\n",
    "  num_samples = 300000\n",
    "  num_features = 25\n",
    "\n",
    "  m = np.random.uniform(-1, 1, size=num_features)  # Slope/coefficients for features\n",
    "  c = np.random.uniform(-5, 5)  # Intercept\n",
    "\n",
    "  data, labels, sensoring = generate_classification_data(m, c, num_samples, num_features, nonlinear=True, nlband=10)\n",
    "  labels = labels.astype(float)\n",
    "  print('labels.shape:', labels.shape, 'labels[sensoring==0].sum():', labels[sensoring==0].sum(), 'sensoring.sum():', sensoring.sum())\n",
    "\n",
    "  risk_rate = sorted(risk_rate, reverse=True)\n",
    "  censoring_rate = sorted(censoring_rate, reverse=True)\n",
    "  n = sorted(n, reverse=True)\n",
    "  \n",
    "  for rr in risk_rate:\n",
    "      for cc in censoring_rate:\n",
    "          print('-----------------------------------------')\n",
    "          X, y, c = filter_data(data.copy(), labels.copy(), sensoring.copy(), n=n[0], p_y=rr, p_c=cc)\n",
    "          X_train, X_test, y_train, y_test, c_train, c_test = train_test_split(X.copy(), y.copy(), c.copy(), test_size=test_size, random_state=0, stratify= y*2 + c)\n",
    "          X_train, X_val, y_train, y_val, c_train, c_val = train_test_split(X_train, y_train, c_train, test_size=test_size, random_state=0, \n",
    "                                                                            stratify= y_train*2 + c_train)\n",
    "          \n",
    "          data_dict['X_train'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = X_train\n",
    "          data_dict['y_train'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = y_train\n",
    "          data_dict['c_train'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = c_train\n",
    "          data_dict['X_val'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = X_val\n",
    "          data_dict['y_val'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = y_val\n",
    "          data_dict['c_val'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = c_val\n",
    "          data_dict['X_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = X_test\n",
    "          data_dict['y_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = y_test\n",
    "          data_dict['c_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = c_test\n",
    "\n",
    "          for i in range(1, len(n)):\n",
    "              # we know total size of dataset whihc has to be split into train/val/test. Further\n",
    "              # desired dataset to be subset of bigger dataset\n",
    "              # check least populated and remove if there is only one sample to avoid splitting error.\n",
    "            #   X_train, y_train, c_train = check_least_populated(X_train, y_train, c_train)\n",
    "            #   X_val, y_val, c_val = check_least_populated(X_val, y_val, c_val)\n",
    "              _, X_train, _, y_train, _, c_train = train_test_split(X_train, y_train, c_train, test_size=int(n[i]*(1-test_size)*(1-test_size)), \n",
    "                                                                    random_state=42, stratify=y_train*2 + c_train)\n",
    "              _, X_val, _, y_val, _, c_val = train_test_split(X_val, y_val, c_val, test_size=int(n[i]*(1-test_size)*test_size), random_state=42, \n",
    "                                                              stratify=y_val*2 + c_val)\n",
    "              print('Rates:', cc, int((c_train.sum())/c_train.shape[0]*100), rr, int((y_train[c_train==0].sum())/y_train[c_train==0].shape[0]*100))\n",
    "              print('n:', n[i], 'train:', int(n[i]*(1-test_size)*(1-test_size)), 'val:', int(n[i]*(1-test_size)*test_size), \n",
    "                    'y val sum:', y_val[c_val==0].sum(), 'c val sum:', c_val.sum(), ' y1c1 val sum:', y_val[c_val==1].sum())\n",
    "              \n",
    "              data_dict['X_train'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = X_train\n",
    "              data_dict['y_train'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = y_train\n",
    "              data_dict['c_train'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = c_train\n",
    "              data_dict['X_val'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = X_val\n",
    "              data_dict['y_val'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = y_val\n",
    "              data_dict['c_val'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = c_val\n",
    "              # test dataset will be same for different n[i]s of train/val data\n",
    "              # but naming convention is different just for programming\n",
    "              data_dict['X_test'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = data_dict['X_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)]\n",
    "              data_dict['y_test'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = data_dict['y_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)]\n",
    "              data_dict['c_test'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = data_dict['c_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)]\n",
    "\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data with 2 features and 100 data points\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "num_samples = 1500\n",
    "num_features = 2\n",
    "\n",
    "m = np.random.uniform(-1, 1, size=num_features)  # Slope/coefficients for features\n",
    "c = np.random.uniform(-5, 5)  # Intercept\n",
    "\n",
    "data, labels, sensoring = generate_classification_data(m, c, num_samples, num_features, nonlinear=True, nlband=10)\n",
    "# filter data as per required risk and censoring\n",
    "# data, labels, sensoring = filter_data(data, labels, sensoring, 2000, .40, .40)\n",
    "\n",
    "print('labels.shape:', labels.shape, 'labels.sum():', labels[sensoring==0].sum(), 'sensoring.sum():', sensoring.sum(), sensoring.sum()/labels.shape[0])\n",
    "\n",
    "# # # Perform t-SNE on the data to reduce it to 2 dimensions for visualization\n",
    "# # tsne = TSNE(n_components=2, random_state=42, perplexity=60)\n",
    "# # data_tsne = tsne.fit_transform(data)\n",
    "# # # Plot the t-SNE transformed data\n",
    "# # plt.figure(figsize=(8, 6))\n",
    "# # plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=labels, cmap='viridis', edgecolors='k')\n",
    "# # plt.title('t-SNE Visualization of Synthetic Data with Labels')\n",
    "# # plt.colorbar(label='Labels')\n",
    "# # plt.grid(True)\n",
    "# # plt.show()\n",
    "\n",
    "# # # labels[(labels == 0) & (sensoring == 0)] = 0\n",
    "# # # labels[(labels == 0) & (sensoring == 1)] = 1\n",
    "# # # labels[(labels == 1) & (sensoring == 0)] = 0\n",
    "# # # labels[(labels == 1) & (sensoring == 1)] = 1\n",
    "# labels[(sensoring == 1)] = 3\n",
    "# # # Plot the data points\n",
    "plt.figure(figsize=(8, 6), frameon=False)\n",
    "# Define a custom mapping between labels and colors\n",
    "color_map = {0: 'red', 1: 'blue', 3: 'yellow'}\n",
    "marker_map = {0: '+', 1: '_', 3:'*'}\n",
    "label_map = {0: 'Low Risk', 1: 'High Risk', 3: 'Sensored'}\n",
    "# plt.scatter(data[:, 0], data[:, 1], c=labels, edgecolors='k')#, label=color_map.keys())#, cmap='viridis')\n",
    "# Add labels for each category\n",
    "for label, color in color_map.items():\n",
    "    # if label==3:\n",
    "    #     continue\n",
    "    plt.scatter(data[labels == label][:, 0], data[labels == label][:, 1], label=label_map[label], marker=marker_map[label], color='black')\n",
    "# plt.scatter(data[labels==0][:, 0], data[labels==0][:, 1], label='live', edgecolors='k')\n",
    "# plt.scatter(data[labels==1][:, 0], data[labels==1][:, 1], label='dead', edgecolors='k')\n",
    "# plt.scatter(data[labels==3][:, 0], data[labels==3][:, 1], label='sensored', edgecolors='k')\n",
    "# plt.title('Generated Data with Binary Feature and Labels')\n",
    "plt.xlabel(None, fontsize=16)\n",
    "plt.ylabel(None, fontsize=16)\n",
    "# plt.colorbar(label='Labels')\n",
    "plt.xticks([], fontsize=14)\n",
    "plt.yticks([],fontsize=14)\n",
    "# plt.legend(fontsize=16, loc='upper right')\n",
    "# plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_dataset_combinations(risk_rate=[.4, .3, .2, .1, 0.05], censoring_rate=[.4, .3, .2, .1, 0.05], n=[5000, 4000, 3000, 2000, 1000, 700])\n",
    "\n",
    "with open('/data/Vinod/Data/datasets/SelectioinBias/data/selection_bias_data.pkl', 'wb') as file:\n",
    "    pickle.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to sample data to study risk rate and censoring\n",
    "def sample_y_c_old(X, y, c, y_r, c_r, scale_cols, test_size=0.2):\n",
    "    # converting ratio to numbers\n",
    "    y_r = [int(X[c==0].shape[0] * (r/(1+r))) for r in y_r]\n",
    "    c_r = [int(X.shape[0] * (r/(1+r))) for r in c_r]\n",
    "\n",
    "    # saving origional data\n",
    "    X_train, X_test, y_train, y_test, c_train, c_test = train_test_split(X, y, c, test_size=test_size, random_state=0, stratify= y*2 + c)\n",
    "    X_train, X_val, y_train, y_val, c_train, c_val = train_test_split(X_train, y_train, c_train, test_size=test_size, random_state=0, \n",
    "                                                                      stratify= y_train*2 + c_train)\n",
    "\n",
    "    # save actual data\n",
    "    data_dict = {}\n",
    "    data_dict['X_train'+str('X')+'R'+str('X')+'C'+str('X')] = X_train\n",
    "    data_dict['y_train'+str('X')+'R'+str('X')+'C'+str('X')] = y_train\n",
    "    data_dict['c_train'+str('X')+'R'+str('X')+'C'+str('X')] = c_train\n",
    "    data_dict['X_val'+str('X')+'R'+str('X')+'C'+str('X')] = X_val\n",
    "    data_dict['y_val'+str('X')+'R'+str('X')+'C'+str('X')] = y_val\n",
    "    data_dict['c_val'+str('X')+'R'+str('X')+'C'+str('X')] = c_val\n",
    "    data_dict['X_test'+str('X')+'R'+str('X')+'C'+str('X')] = X_test\n",
    "    data_dict['y_test'+str('X')+'R'+str('X')+'C'+str('X')] = y_test\n",
    "    data_dict['c_test'+str('X')+'R'+str('X')+'C'+str('X')] = c_test\n",
    "    \n",
    "    # sort in descending order\n",
    "    y_r = sorted(y_r, reverse=True)\n",
    "    c_r = sorted(c_r, reverse=True)\n",
    "    \n",
    "    for c_r_i in c_r:\n",
    "        for y_r_i in y_r:\n",
    "            X, y, c = sample_censoring(X.copy(), y.copy(), c.copy(), c_r_i)\n",
    "            X, y, c = sample_yrate(X.copy(), y.copy(), c.copy(), y_r_i)\n",
    "\n",
    "            # split train-val-test,\n",
    "            X_train, X_test, y_train, y_test, c_train, c_test = train_test_split(X, y, c, test_size=test_size, random_state=0, stratify= y*2 + c)\n",
    "            X_train, X_val, y_train, y_val, c_train, c_val = train_test_split(X_train, y_train, c_train, test_size=test_size, random_state=0, \n",
    "                                                                            stratify= y_train*2 + c_train)\n",
    "            \n",
    "            # scale data\n",
    "            # scale_cols = ['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education','Income']\n",
    "            scaler = RobustScaler().fit(X_train[scale_cols])\n",
    "            X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n",
    "            X_val[scale_cols] = scaler.transform(X_val[scale_cols])\n",
    "            X_test[scale_cols] = scaler.transform(X_test[scale_cols])\n",
    "\n",
    "            # and put in dictionary\n",
    "            data_dict['X_train'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = X_train\n",
    "            data_dict['y_train'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = y_train\n",
    "            data_dict['c_train'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = c_train\n",
    "            data_dict['X_val'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = X_val\n",
    "            data_dict['y_val'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = y_val\n",
    "            data_dict['c_val'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = c_val\n",
    "            data_dict['X_test'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = X_test\n",
    "            data_dict['y_test'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = y_test\n",
    "            data_dict['c_test'+str('X')+'R'+str('y_r_i')+'C'+str('c_r_i')] = c_test\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def create_bias(X, seed, threshold, bins=100, plot=False):\n",
    "    np.random.seed(seed)\n",
    "    w = 2 * np.random.rand(X.shape[1]) - 1\n",
    "    temp = subtract_mean_from_each_column(X.copy())\n",
    "    temp = (temp * w[None,:]).sum(axis=1)\n",
    "    temp_std = temp.std(axis=0)\n",
    "\n",
    "    temp = 4 * temp / temp_std\n",
    "    temp = 1 / (1 + np.exp(-temp))\n",
    "\n",
    "    c = (temp>threshold).astype(int)\n",
    "\n",
    "    if plot:\n",
    "        plt.hist(temp, bins=bins)\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "    return c\n",
    "\n",
    "def subtract_mean_from_each_column(df):\n",
    "    for col in df.columns:\n",
    "        df[col] -= df[col].mean()\n",
    "    return df\n",
    "\n",
    "def sample_censoring(X, y, c, n):\n",
    "    # separate censored units\n",
    "    X_c = X[c==1]\n",
    "    y_c = y[c==1]\n",
    "    indices = np.arange(len(X_c))\n",
    "    np.random.shuffle(indices)\n",
    "    X_c = X_c.iloc[indices[:n]]\n",
    "    y_c = y_c.iloc[indices[:n]]\n",
    "\n",
    "    X = pd.concat([X[c!=1], X_c], ignore_index=True)\n",
    "    y = pd.concat([y[c!=1], y_c], ignore_index=True)\n",
    "    c = c[:X.shape[0]]\n",
    "    c.iloc[:-X_c.shape[0]] = 0\n",
    "    c.iloc[-X_c.shape[0]:] = 1\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X.iloc[indices].reset_index(drop=True)\n",
    "    y = y.iloc[indices].reset_index(drop=True)\n",
    "    c = c.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "    return X, y, c\n",
    "\n",
    "def sample_yrate(X, y, c, n):\n",
    "    X_y = X[(y==1) & (c==0)]\n",
    "    c_y = c[(y==1) & (c==0)]\n",
    "    indices = np.arange(len(X_y))\n",
    "    np.random.shuffle(indices)\n",
    "    X_y = X_y.iloc[indices[:n]]\n",
    "    c_y = c_y.iloc[indices[:n]]\n",
    "\n",
    "    X = pd.concat([X[~((y==1) & (c==0))], X_y], ignore_index=True)\n",
    "    c = pd.concat([c[~((y==1) & (c==0))], c_y], ignore_index=True)\n",
    "    y = y[:X.shape[0]]\n",
    "    y[:-X_y.shape[0]] = 0\n",
    "    y[-X_y.shape[0]:] = 1\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X.iloc[indices].reset_index(drop=True)\n",
    "    y = y.iloc[indices].reset_index(drop=True)\n",
    "    c = c.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "    return X, y, c\n",
    "\n",
    "def sample_datasets(X, y, c, y_r, c_r, n, scale_cols, test_size=0.2):\n",
    "    '''\n",
    "    This function will create datasets with different combinations of rates and sizes.\n",
    "        X - dataframe, containing features\n",
    "        y - labels\n",
    "        c - censoring indicator\n",
    "        n - is total number of data points\n",
    "        c_r - rate of censoring and calculated on n\n",
    "        y_r - event rate and calculated on uncensored data (i.e., after removing censored units)\n",
    "        scale_cols - list of features to be scaled\n",
    "        test_size - test and validation split fraction\n",
    "\n",
    "    '''\n",
    "    # saving origional data\n",
    "    X_train, X_test, y_train, y_test, c_train, c_test = train_test_split(X.copy(), y.copy(), c.copy(), test_size=test_size, random_state=0, stratify= y*2 + c)\n",
    "    X_train, X_val, y_train, y_val, c_train, c_val = train_test_split(X_train, y_train, c_train, test_size=test_size, random_state=0, \n",
    "                                                                      stratify= y_train*2 + c_train)\n",
    "    # scale data\n",
    "    # print(X_train[scale_cols])\n",
    "    scaler = RobustScaler().fit(X_train[scale_cols])\n",
    "    X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n",
    "    X_val[scale_cols] = scaler.transform(X_val[scale_cols])\n",
    "    X_test[scale_cols] = scaler.transform(X_test[scale_cols])\n",
    "\n",
    "    # save actual data\n",
    "    data_dict = {}\n",
    "    data_dict['X_train'+str('X')+'R'+str('X')+'C'+str('X')] = X_train.values\n",
    "    data_dict['y_train'+str('X')+'R'+str('X')+'C'+str('X')] = y_train.values\n",
    "    data_dict['c_train'+str('X')+'R'+str('X')+'C'+str('X')] = c_train.values\n",
    "    data_dict['X_val'+str('X')+'R'+str('X')+'C'+str('X')] = X_val.values\n",
    "    data_dict['y_val'+str('X')+'R'+str('X')+'C'+str('X')] = y_val.values\n",
    "    data_dict['c_val'+str('X')+'R'+str('X')+'C'+str('X')] = c_val.values\n",
    "    data_dict['X_test'+str('X')+'R'+str('X')+'C'+str('X')] = X_test.values\n",
    "    data_dict['y_test'+str('X')+'R'+str('X')+'C'+str('X')] = y_test.values\n",
    "    data_dict['c_test'+str('X')+'R'+str('X')+'C'+str('X')] = c_test.values\n",
    "    \n",
    "    # sort in descending order\n",
    "    y_r = sorted(y_r, reverse=True)\n",
    "    c_r = sorted(c_r, reverse=True)\n",
    "    n = sorted(n, reverse=True)\n",
    "    \n",
    "    for rr in y_r:\n",
    "      for cc in c_r:\n",
    "          print('------------------------------------')\n",
    "          X1, y1, c1 = filter_data(X.copy(), y.copy(), c.copy(), n=n[0], p_y=rr, p_c=cc, dataframe=True)\n",
    "        #   print('Rates:', cc, int((c1.sum())/n[0]*100), rr, int((y1[c1==0].sum())/y1[c1==0].shape[0]*100))\n",
    "\n",
    "          X_train, X_test, y_train, y_test, c_train, c_test = train_test_split(X1, y1, c1, test_size=test_size, random_state=0, stratify= y1*2 + c1)\n",
    "          X_train, X_val, y_train, y_val, c_train, c_val = train_test_split(X_train, y_train, c_train, test_size=test_size, random_state=0, \n",
    "                                                                            stratify= y_train*2 + c_train)\n",
    "        #   print('Rates:', cc, int((c_train.sum())/c_train.shape[0]*100), rr, int((y_train[c_train==0].sum())/y_train[c_train==0].shape[0]*100))\n",
    "        #   print('Rates:', n[0], c_train.shape[0], c_val.shape[0], cc, int((c_val.sum())/c_val.shape[0]*100), int((c_val.sum())), rr, int((y_val[c_val==0].sum())/y_val[c_val==0].shape[0]*100),  int((y_train[c_train==0].sum())))\n",
    "          # scale data\n",
    "          scaler = RobustScaler().fit(X_train[scale_cols])\n",
    "          X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n",
    "          X_val[scale_cols] = scaler.transform(X_val[scale_cols])\n",
    "          X_test[scale_cols] = scaler.transform(X_test[scale_cols])\n",
    "          \n",
    "          data_dict['X_train'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = X_train.values\n",
    "          data_dict['y_train'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = y_train.values\n",
    "          data_dict['c_train'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = c_train.values\n",
    "          data_dict['X_val'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = X_val.values\n",
    "          data_dict['y_val'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = y_val.values\n",
    "          data_dict['c_val'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = c_val.values\n",
    "          data_dict['X_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = X_test.values\n",
    "          data_dict['y_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = y_test.values\n",
    "          data_dict['c_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)] = c_test.values\n",
    "          \n",
    "          for i in range(1, len(n)):\n",
    "              # we know total size of dataset whihc has to be split into train/val/test. Further\n",
    "              # desired dataset to be subset of bigger dataset\n",
    "              # check least populated and remove if there is only one sample to avoid splitting error.\n",
    "              X_train, y_train, c_train = check_least_populated(X_train, y_train, c_train)\n",
    "              X_val, y_val, c_val = check_least_populated(X_val, y_val, c_val)\n",
    "              \n",
    "            #   print(X_train.shape, n[i-1], int(n[i-1]*(1-test_size)*(1-test_size)), int(n[i]*(1-test_size)*(1-test_size)))\n",
    "              _, X_train, _, y_train, _, c_train = train_test_split(X_train, y_train, c_train, test_size=int(n[i]*(1-test_size)*(1-test_size)), \n",
    "                                                                    random_state=42, stratify=y_train*2 + c_train)\n",
    "              _, X_val, _, y_val, _, c_val = train_test_split(X_val, y_val, c_val, test_size=int(n[i]*(1-test_size)*test_size), \n",
    "                                                              random_state=42, stratify=y_val*2 + c_val)\n",
    "              print('Rates:', cc, int((c_train.sum())/c_train.shape[0]*100), rr, int((y_train[c_train==0].sum())/y_train[c_train==0].shape[0]*100))\n",
    "              print('n:', n[i], 'train:', int(n[i]*(1-test_size)*(1-test_size)), 'val:', int(n[i]*(1-test_size)*test_size), \n",
    "                    'y val sum:', y_val[c_val==0].sum(), 'c val sum:', c_val.sum(), ' y1c1 val sum:', y_val[c_val==1].sum())\n",
    "              \n",
    "              data_dict['X_train'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = X_train.values\n",
    "              data_dict['y_train'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = y_train.values\n",
    "              data_dict['c_train'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = c_train.values\n",
    "              data_dict['X_val'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = X_val.values\n",
    "              data_dict['y_val'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = y_val.values\n",
    "              data_dict['c_val'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = c_val.values\n",
    "              # test dataset will be same for different n[i]s of train/val data\n",
    "              # but naming convention is different just for programming\n",
    "              data_dict['X_test'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = data_dict['X_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)]\n",
    "              data_dict['y_test'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = data_dict['y_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)]\n",
    "              data_dict['c_test'+str(n[i])+'R'+str(rr)+'C'+str(cc)] = data_dict['c_test'+str(n[0])+'R'+str(rr)+'C'+str(cc)]\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data\n",
    "with open('paths.json', 'r') as json_file:\n",
    "    file_path = json.load(json_file)['diabetes_binary_5050split_health_indicators_BRFSS2015.csv']\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "y = df['Diabetes_binary']\n",
    "X = df.drop(['Diabetes_binary'], axis=1)\n",
    "c = create_bias(X, seed=44, threshold=0.8)\n",
    "print(X.shape, y.shape, y[c==0].sum()/y[c==0].shape[0], c.sum()/c.shape[0])\n",
    "\n",
    "scale_cols = ['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education','Income']\n",
    "data_dict = sample_datasets(X, y, c, y_r=[.4, .3, .2, .1, 0.05], c_r=[.4, .3, .2, .1, 0.05], n=[25000, 10000, 5000, 2000, 1000, 700], scale_cols=scale_cols, test_size=0.2)\n",
    "\n",
    "# # check dataset sizes\n",
    "# print(data_dict['X_train700R0.1C0.1'].shape[0]+data_dict['X_val700R0.1C0.1'].shape[0]+data_dict['X_test700R0.1C0.1'].shape[0])\n",
    "# print(data_dict['c_train700R0.1C0.1'].sum()+data_dict['c_val700R0.1C0.1'].sum()+data_dict['c_test700R0.1C0.1'].sum())\n",
    "# print(data_dict['y_train700R0.1C0.1'].sum()+data_dict['y_val700R0.1C0.1'].sum()+data_dict['y_test700R0.1C0.1'].sum())\n",
    "# print(data_dict['y_train700R0.1C0.1'][data_dict['c_train700R0.1C0.1']==0].sum()+data_dict['y_val700R0.1C0.1'][data_dict['c_val700R0.1C0.1']==0].sum()+data_dict['y_test700R0.1C0.1'][data_dict['c_test700R0.1C0.1']==0].sum())\n",
    "\n",
    "# save\n",
    "with open('paths.json', 'r') as json_file:\n",
    "    file_path = json.load(json_file)['diabetes_bias_data.pkl']\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid-19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values_per_column(df):\n",
    "  \"\"\"\n",
    "  This function finds the unique values for each column in a pandas DataFrame.\n",
    "\n",
    "  Args:\n",
    "      df: A pandas DataFrame.\n",
    "\n",
    "  Returns:\n",
    "      A dictionary where the keys are the column names and the values are lists of unique values for each column.\n",
    "  \"\"\"\n",
    "  unique_values = {}\n",
    "  for col in df.columns:\n",
    "    unique_values[col] = df[col].unique()\n",
    "    print(col, df[col].unique())\n",
    "  return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/tanmoyx/covid19-patient-precondition-dataset/data\n",
    "with open('paths.json', 'r') as json_file:\n",
    "    file_path = json.load(json_file)['covid_processed.csv']\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# result = unique_values_per_column(df.copy())\n",
    "# print(result)\n",
    "\n",
    "# changing range to 0-1 from 1-2\n",
    "df['intubed'] = df['intubed'] -1\n",
    "df['pneumonia'] = df['pneumonia'] -1\n",
    "df['pregnancy'] = df['pregnancy'] -1\n",
    "df['diabetes'] = df['diabetes'] -1\n",
    "df['copd'] = df['copd'] -1\n",
    "df['asthma'] = df['asthma'] -1\n",
    "df['inmsupr'] = df['inmsupr'] -1\n",
    "df['hypertension'] = df['hypertension'] -1\n",
    "df['other_disease'] = df['other_disease'] -1\n",
    "df['cardiovascular'] = df['cardiovascular'] -1\n",
    "df['obesity'] = df['obesity'] -1\n",
    "df['renal_chronic'] = df['renal_chronic'] -1\n",
    "df['tobacco'] = df['tobacco'] -1\n",
    "df['contact_other_covid'] = df['contact_other_covid'] -1\n",
    "df['icu'] = df['icu'] -1\n",
    "\n",
    "# result = unique_values_per_column(df.copy())\n",
    "# print(result)\n",
    "\n",
    "# since +ve class is more so inverting y\n",
    "y = 1 - df['Chance']\n",
    "X = df.drop(['Chance'], axis=1)\n",
    "# creating bias in the data\n",
    "c = create_bias(X, seed=42, threshold=0.895)\n",
    "print(X.shape, y.shape, y[c==0].sum()/y[c==0].shape[0], c.sum()/c.shape[0])\n",
    "\n",
    "# sample and create datasets\n",
    "scale_cols = ['age']\n",
    "data_dict = sample_datasets(X, y, c, y_r=[.4, .3, .2, .1, 0.05], c_r=[.4, .3, .2, .1, 0.05], n=[15000, 10000, 5000, 2000, 1000, 700], scale_cols=scale_cols, test_size=0.2)\n",
    "\n",
    "# save data\n",
    "with open('paths.json', 'r') as json_file:\n",
    "    file_path = json.load(json_file)['covid_bias_data.pkl']\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(data_dict, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
