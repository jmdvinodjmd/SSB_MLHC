{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve Selection Bias by identifying sensoring in two phase process\n",
    "\n",
    "- Train risk prediction model on biased data\n",
    "- Train sensoring prediction model\n",
    "- Use sensoring prediction model to identify units and use risk prediction for identified units only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T02:38:36.219821Z",
     "iopub.status.busy": "2024-02-09T02:38:36.219395Z",
     "iopub.status.idle": "2024-02-09T02:38:39.719568Z",
     "shell.execute_reply": "2024-02-09T02:38:39.718601Z"
    }
   },
   "outputs": [],
   "source": [
    "# set working directory\n",
    "from random import SystemRandom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "# from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from torch._C import float32\n",
    "import argparse\n",
    "from asyncio.log import logger\n",
    "import os, math\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Importing matplotlib and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "# import tabulate\n",
    "\n",
    "import utils\n",
    "from utils import *\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T02:38:39.724809Z",
     "iopub.status.busy": "2024-02-09T02:38:39.723963Z",
     "iopub.status.idle": "2024-02-09T02:38:39.802771Z",
     "shell.execute_reply": "2024-02-09T02:38:39.801937Z"
    }
   },
   "outputs": [],
   "source": [
    "MINI_BATCH = 64\n",
    "EPOCHS = 1000\n",
    "LOAD = None\n",
    "SEED = 42\n",
    "REPEAT = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Set device to GPU\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Set device to CPU\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "param_grid = {\n",
    "            'drop_rate': [0.1],\n",
    "            'hidden_sizes':[[50], [100], [100, 100]],# [[50], [50, 50], [50, 100], [100], [100, 100]],\n",
    "            'head_sizes':[[50], [100]],# [[50], [100]],\n",
    "            'lr':[0.0001, 0.0005]\n",
    "        }\n",
    "# params_risk = {\n",
    "#             'drop_rate': 0.05,\n",
    "#             'hidden_sizes':[100, 100],# [[50], [50, 50], [50, 100], [100], [100, 100]],\n",
    "#             'head_sizes':[50],# [[50], [100]],\n",
    "#             'lr':0.0001\n",
    "#         }\n",
    "# params_censoring = params_risk \n",
    "############################################\n",
    "# initilising wandb\n",
    "# wandb.init(project='SeletionBML', entity=\"jmdvinodjmd\")\n",
    "wandb.init(mode=\"disabled\")\n",
    "wandb.run.name = 'SB'\n",
    "makedirs('./results/')\n",
    "experimentID = LOAD\n",
    "if experimentID is None:\n",
    "    experimentID = int(SystemRandom().random()*100000)\n",
    "# checkpoint\n",
    "ckpt_path = os.path.join('./results/checkpoints/TNet_model.ckpt')\n",
    "makedirs('./results/checkpoints/')\n",
    "# set logger\n",
    "log_path = os.path.join(\"./results/logs/\" + \"exp_TNet_\" + str(experimentID) + \".log\")\n",
    "makedirs(\"./results/logs/\")\n",
    "logger = get_logger(logpath=log_path, filepath=\"exp_TNet_\" + str(experimentID) + \".log\", displaying=False)\n",
    "logger.info(\"Experiment \" + str(experimentID))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T02:38:39.807501Z",
     "iopub.status.busy": "2024-02-09T02:38:39.807324Z",
     "iopub.status.idle": "2024-02-09T02:38:39.833312Z",
     "shell.execute_reply": "2024-02-09T02:38:39.832274Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(data, params_risk, params_censoring, repeat=1):\n",
    "    [X_train, y_train, s_train, X_val, y_val, s_val, X_test, y_test, s_test] = data\n",
    "\n",
    "    # data for risk prediction\n",
    "    loader_train_br, input_size = get_loaders([X_train[s_train==0], y_train[s_train==0]], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "    loader_val_br, _ = get_loaders([X_val[s_val==0], y_val[s_val==0]], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "    loader_test_br, _ = get_loaders([X_test[s_test==0], y_test[s_test==0]], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "    loader_test_ur, _ = get_loaders([X_test, y_test], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "\n",
    "    # data for sensoring prediction\n",
    "    loader_train_c, input_size = get_loaders([X_train, s_train], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "    loader_val_c, _ = get_loaders([X_val, s_val], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "    loader_test_c, _ = get_loaders([X_test, s_test], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "\n",
    "    # repeating experiment for a given number of times\n",
    "    results_risk = {}\n",
    "    for i in range(repeat):\n",
    "        logger.info('Repeating: ' + str(i+1))\n",
    "        results_risk[i] = {}\n",
    "        #############################\n",
    "        # train risk prediction model\n",
    "        model_risk, optimizer, criterion = create_model('MLP', params_risk, input_size, output_size=1, device=device)\n",
    "        early_stopping = EarlyStopping(patience=10, path=ckpt_path, verbose=True, logger=logger)\n",
    "        logger.info(model_risk)\n",
    "        wandb.watch(model_risk)\n",
    "        # train\n",
    "        model_risk = train_model(model_risk, 'MLP', loader_train_br, loader_val_br, optimizer, criterion, early_stopping, logger, epochs=EPOCHS, plot=False, wandb=wandb)\n",
    "        # evaluate\n",
    "        auroc_vb, _ = evaluate_model('Val', loader_val_br, model_risk, 'MLP', criterion, logger, -1, device, wandb)\n",
    "        auroc_tb, _ = evaluate_model('Test', loader_test_br, model_risk, 'MLP', criterion, logger, -1, device, wandb)\n",
    "        auroc_tu, _ = evaluate_model('Test', loader_test_ur, model_risk, 'MLP', criterion, logger, -1, device, wandb)\n",
    "\n",
    "        logger.info('Risk Prediction: biased Val AUROC:' + str(auroc_vb['Val AUROC']) + ' biased Test AUROC:' + str(auroc_tb['Test AUROC']) + ' unbiased Test AUROC:' + str(auroc_tu['Test AUROC']))\n",
    "        results_risk[i] = {'R-Val AUROC':auroc_vb['Val AUROC'], 'R-Test AUROC-B':auroc_tb['Test AUROC'], 'R-Test AUROC-U':auroc_tu['Test AUROC']}\n",
    "\n",
    "        ############################\n",
    "        # train sensoring prediction model\n",
    "        model_sensoring, optimizer, criterion = create_model('MLP', params_censoring, input_size, output_size=1, device=device)\n",
    "        early_stopping = EarlyStopping(patience=10, path=ckpt_path, verbose=True, logger=logger)\n",
    "        logger.info(model_sensoring)\n",
    "        wandb.watch(model_sensoring)\n",
    "        # train\n",
    "        model_sensoring = train_model(model_sensoring, 'MLP', loader_train_c, loader_val_c, optimizer, criterion, early_stopping, logger, epochs=EPOCHS, plot=False, wandb=wandb)\n",
    "        # evaluate\n",
    "        auroc_v, best_threshold = evaluate_model('Val', loader_val_c, model_sensoring, 'MLP', criterion, logger, -1, device, wandb)\n",
    "        auroc_t, _ = evaluate_model('Test', loader_test_c, model_sensoring, 'MLP', criterion, logger, -1, device, wandb)\n",
    "        logger.info('Sensoring: Val AUROC:' + str(auroc_v['Val AUROC']) + ' Test AUROC:' + str(auroc_t['Test AUROC']))\n",
    "        results_risk[i].update({'C-Val AUROC':auroc_v['Val AUROC'], 'C-Test AUROC':auroc_t['Test AUROC']})\n",
    "\n",
    "        ############################\n",
    "        # Identify sensored and predict risk for unsensored\n",
    "        sensored_units = (model_sensoring(torch.tensor(X_test, dtype=torch.float).to(device)) > torch.tensor(best_threshold, dtype=torch.float).to(device)).cpu().numpy().astype(int)\n",
    "        loader_utest, _ = get_loaders([X_test[sensored_units.squeeze()==0], y_test[sensored_units.squeeze()==0]], batch_size=y_test.shape[0], is_train=False, device=device)\n",
    "\n",
    "        # check if all censored or none\n",
    "        if sensored_units.sum()==0 or sensored_units.sum()==X_test.shape[0]:\n",
    "            logger.info('all censored or none. Predicted Sensored:' + str(sensored_units.sum()) + ' Actual Sensored:' + str(s_test.sum()))\n",
    "            auroc = {'TB-EU:Identify AUROC':-1}\n",
    "        else:\n",
    "            logger.info('sizes:'+str(X_test[sensored_units.squeeze()==0].shape)+str(X_test.shape)+'---------------')\n",
    "            logger.info('sizes:'+str(y_test[sensored_units.squeeze()==0].shape)+str(y_test[sensored_units.squeeze()==0].sum())+'---------------')\n",
    "            auroc, _ = evaluate_model('TB-EU:Identify', loader_utest, model_risk, 'MLP', criterion, logger, -1, device, wandb)\n",
    "\n",
    "        logger.info('TB-EU:Identify AUROC:' + str(auroc['TB-EU:Identify AUROC']) + '. Unensored/Total:' + str(X_test[sensored_units.squeeze()==0].shape[0])+ '/'+str(X_test.shape[0])\\\n",
    "                    +'. Actual Unensored/Total:' + str(X_test[s_test.squeeze()==0].shape[0])+ '/'+str(X_test.shape[0]))\n",
    "        results_risk[i].update({'R-Test-2M':auroc['TB-EU:Identify AUROC'], 'Predicted Sensored':sensored_units.sum(), 'Actual Sensored':s_test.sum()})\n",
    "    \n",
    "    return results_risk\n",
    "\n",
    "def study_effect(data_name, file_name, results_file, r, c, n, search_param=False):\n",
    "    ''' \n",
    "    This function is used to study effect of (riks rate, dataset size etc.).\n",
    "    It expects a set of datasets with some variations.\n",
    "    '''\n",
    "    logger.info('\\n\\n-------------N:'+str(n)+'--Risk Rate:' + str(r)+'--Censoring Rate:' + str(c)+'-------------------------.')\n",
    "\n",
    "    results_sizes = {}\n",
    "    for ni in n:\n",
    "        for ci in c:\n",
    "            for ri in r:\n",
    "                # load data dictionary\n",
    "                data_dict = get_data_dict(file_name, [ri], [ci], [ni])\n",
    "\n",
    "                logger.info('-----Running for Size:'+str(ni)+'--Risk Rate:' + str(ri)+'--Censoring Rate:' + str(ci)+'\\n-----------')\n",
    "                [X_train, y_train, s_train, X_val, y_val, s_val, X_test, y_test, s_test] = data_dict[str(ni)+'R'+str(ri)+'C'+str(ci)]\n",
    "                data = [X_train, y_train, s_train, X_val, y_val, s_val, X_test, y_test, s_test]\n",
    "                \n",
    "                #############################\n",
    "                # Reading hyperparameters from the JSON file\n",
    "                with open('best_hyperparams.json', 'r') as json_file:\n",
    "                    best_hyperparams = json.load(json_file)\n",
    "                if ('TNet-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci) not in best_hyperparams) or search_param:\n",
    "                    # hyperparameter tuning \n",
    "                    logger.info('Finding best hyperparams.')\n",
    "                    loader_train_br, input_size = get_loaders([X_train[s_train==0], y_train[s_train==0]], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "                    loader_val_br, _ = get_loaders([X_val[s_val==0], y_val[s_val==0]], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "                    params_risk, best_score, results = grid_search_MLP('MLP', loader_train_br, loader_val_br, input_size, ckpt_path, param_grid, EPOCHS, logger, wandb, device)\n",
    "                    logger.info('Hyperparam tuning for risk prediction:')\n",
    "                    logger.info(results)\n",
    "\n",
    "                    loader_train_c, input_size = get_loaders([X_train, s_train], batch_size=MINI_BATCH, is_train=True, device=device)\n",
    "                    loader_val_c, _ = get_loaders([X_val, s_val], batch_size=MINI_BATCH, is_train=False, device=device)\n",
    "                    params_censoring, best_score, results = grid_search_MLP('MLP', loader_train_c, loader_val_c, input_size, ckpt_path, param_grid, EPOCHS, logger, wandb, device)\n",
    "                    logger.info('Hyperparam tuning for censoring prediction:')\n",
    "                    logger.info(results)\n",
    "\n",
    "                    best_hyperparams['TNet-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci)] = {'params_risk': params_risk,\n",
    "                                                                          'params_censoring': params_censoring}\n",
    "                    # save best params\n",
    "                    with open('best_hyperparams.json', 'w') as json_file:\n",
    "                        json.dump(best_hyperparams, json_file)\n",
    "                    \n",
    "                else:\n",
    "                    logger.info('Accessing the existing best hyperparams.')\n",
    "                    params_risk = best_hyperparams['TNet-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci)]['params_risk']\n",
    "                    params_censoring = best_hyperparams['TNet-'+data_name+str(ni)+'R'+str(ri)+'C'+str(ci)]['params_censoring']\n",
    "\n",
    "                ################################\n",
    "                # run experiments and repeat for given number of times\n",
    "                results = experiment(data, params_risk, params_censoring, repeat=REPEAT)\n",
    "                logger.info('\\n\\nBest params for risk:\\n' + str(params_risk))\n",
    "                logger.info('Best params for censoring:\\n' + str(params_censoring))\n",
    "                logger.info(results)\n",
    "                results_sizes[str(ni)+'R'+str(ri)+'C'+str(ci)] = results\n",
    "\n",
    "                # save results\n",
    "                dict_to_file(results_file, results_sizes)\n",
    "                ################################\n",
    "\n",
    "    logger.info('\\n\\n------------------- Experiments ended-------------------.\\n'+str(results_sizes)+'\\n------------------------------------------------\\n\\n')\n",
    "\n",
    "    return results_sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T02:38:39.849785Z",
     "iopub.status.busy": "2024-02-09T02:38:39.849371Z",
     "iopub.status.idle": "2024-02-09T02:38:39.853570Z",
     "shell.execute_reply": "2024-02-09T02:38:39.852850Z"
    }
   },
   "outputs": [],
   "source": [
    "results_sizes = study_effect('synthetic', 'selection_bias_data.pkl', 'results_TNet', r=[.05, .1, .2, .3, .4], c=[.05, .1, .2, .3, .4], n=[1000, 2000, 3000, 4000, 5000], search_param=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T02:38:39.892863Z",
     "iopub.status.busy": "2024-02-09T02:38:39.892657Z",
     "iopub.status.idle": "2024-02-09T21:32:30.239452Z",
     "shell.execute_reply": "2024-02-09T21:32:30.238340Z"
    }
   },
   "outputs": [],
   "source": [
    "results_sizes = study_effect('diabetes', 'diabetes_bias_data.pkl', 'results_TNet-diabetes', r=[.05, .1, .2, .3, .4], c=[.05, .1, .2, .3, .4], n=[25000, 10000, 5000, 2000, 1000], search_param=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T21:32:30.254654Z",
     "iopub.status.busy": "2024-02-09T21:32:30.254072Z",
     "iopub.status.idle": "2024-02-09T21:32:30.258008Z",
     "shell.execute_reply": "2024-02-09T21:32:30.257245Z"
    }
   },
   "outputs": [],
   "source": [
    "results_sizes = study_effect('covid', 'covid_bias_data.pkl', 'results_TNet-covid', r=[.05, .1, .2, .3, .4], c=[.05, .1, .2, .3, .4], n=[15000, 10000, 5000, 2000, 1000], search_param=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
